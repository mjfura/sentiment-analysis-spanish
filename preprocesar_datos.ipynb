{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/marcofura/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marcofura/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "df_posts = pd.read_csv(\"posts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizacion de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                post  sentimiento\n",
      "0  Hoy disfruté de un delicioso almuerzo con amigos.            1\n",
      "1         Estoy tan emocionado por mi próximo viaje.            1\n",
      "2  Qué día tan hermoso para salir a caminar por e...            1\n",
      "3  Mi corazón está roto. No puedo creer que haya ...           -1\n",
      "4  ¡Acabo de conseguir un nuevo trabajo! ¡Estoy t...            1\n"
     ]
    }
   ],
   "source": [
    "print(df_posts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens del post 1: ['hoy', 'disfruté', 'de', 'un', 'delicioso', 'almuerzo', 'con', 'amigos', '.']\n",
      "Tokens del post 2: ['estoy', 'tan', 'emocionado', 'por', 'mi', 'próximo', 'viaje', '.']\n",
      "Tokens del post 3: ['qué', 'día', 'tan', 'hermoso', 'para', 'salir', 'a', 'caminar', 'por', 'el', 'parque', '.']\n",
      "Tokens del post 4: ['mi', 'corazón', 'está', 'roto', '.', 'no', 'puedo', 'creer', 'que', 'haya', 'terminado', 'así', '.']\n",
      "Tokens del post 5: ['¡acabo', 'de', 'conseguir', 'un', 'nuevo', 'trabajo', '!', '¡estoy', 'tan', 'feliz', '!']\n",
      "Tokens del post 6: ['la', 'vida', 'a', 'veces', 'puede', 'ser', 'difícil', ',', 'pero', 'sigo', 'adelante', '.']\n",
      "Tokens del post 7: ['estoy', 'cansado', 'de', 'todo', 'el', 'drama', 'en', 'las', 'redes', 'sociales', '.']\n",
      "Tokens del post 8: ['¡felicidades', 'a', 'mi', 'mejor', 'amigo', 'por', 'su', 'boda', '!']\n",
      "Tokens del post 9: ['¡hoy', 'es', 'mi', 'cumpleaños', 'y', 'estoy', 'emocionado', 'por', 'lo', 'que', 'el', 'año', 'nuevo', 'traerá', '!']\n",
      "Tokens del post 10: ['este', 'año', 'ha', 'sido', 'un', 'desafío', ',', 'pero', 'estoy', 'agradecido', 'por', 'todas', 'las', 'lecciones', 'aprendidas', '.']\n",
      "Tokens del post 11: ['la', 'comida', 'en', 'este', 'restaurante', 'es', 'increíble', '.', 'definitivamente', 'volveré', '.']\n",
      "Tokens del post 12: ['¿alguien', 'más', 'está', 'emocionado', 'por', 'la', 'nueva', 'temporada', 'de', 'su', 'serie', 'favorita', '?']\n",
      "Tokens del post 13: ['perdí', 'a', 'mi', 'mascota', 'hoy', '.', 'estoy', 'devastado', '.']\n",
      "Tokens del post 14: ['¡finalmente', 'terminé', 'mi', 'proyecto', 'y', 'estoy', 'orgulloso', 'del', 'resultado', 'final', '!']\n",
      "Tokens del post 15: ['estoy', 'abrumado', 'por', 'todo', 'lo', 'que', 'tengo', 'que', 'hacer', '.']\n",
      "Tokens del post 16: ['¡hoy', 'es', 'un', 'día', 'de', 'fiesta', 'nacional', 'en', 'mi', 'país', '!', '¡vamos', 'a', 'celebrar', '!']\n",
      "Tokens del post 17: ['¿alguien', 'tiene', 'recomendaciones', 'para', 'libros', 'interesantes', '?']\n",
      "Tokens del post 18: ['me', 'siento', 'agradecido', 'por', 'las', 'pequeñas', 'cosas', 'en', 'la', 'vida', '.']\n",
      "Tokens del post 19: ['la', 'música', 'siempre', 'me', 'levanta', 'el', 'ánimo', '.']\n",
      "Tokens del post 20: ['odio', 'estar', 'enfermo', '.', 'espero', 'recuperarme', 'pronto', '.']\n",
      "Tokens del post 21: ['acabo', 'de', 'ver', 'una', 'película', 'que', 'me', 'dejó', 'pensando', 'durante', 'horas', '.']\n",
      "Tokens del post 22: ['¡hoy', 'es', 'un', 'día', 'de', 'descanso', 'bien', 'merecido', '!']\n",
      "Tokens del post 23: ['las', 'risas', 'con', 'amigos', 'son', 'siempre', 'las', 'mejores', '.']\n",
      "Tokens del post 24: ['estoy', 'nervioso', 'por', 'la', 'entrevista', 'de', 'trabajo', 'de', 'mañana', '.']\n",
      "Tokens del post 25: ['la', 'lluvia', 'siempre', 'me', 'pone', 'de', 'buen', 'humor', '.']\n",
      "Tokens del post 26: ['¡qué', 'hermoso', 'atardecer', '!', 'no', 'puedo', 'resistirme', 'a', 'tomar', 'una', 'foto', '.']\n",
      "Tokens del post 27: ['las', 'noticias', 'de', 'hoy', 'me', 'han', 'dejado', 'sin', 'palabras', '.']\n",
      "Tokens del post 28: ['¡es', 'viernes', '!', '¡vamos', 'a', 'divertirnos', 'este', 'fin', 'de', 'semana', '!']\n",
      "Tokens del post 29: ['mi', 'libro', 'favorito', 'acaba', 'de', 'lanzar', 'una', 'nueva', 'secuela', '.', '¡no', 'puedo', 'esperar', 'para', 'leerlo', '!']\n",
      "Tokens del post 30: ['mi', 'corazón', 'está', 'lleno', 'de', 'gratitud', 'por', 'todas', 'las', 'personas', 'increíbles', 'en', 'mi', 'vida', '.']\n",
      "Tokens del post 31: ['me', 'siento', 'un', 'poco', 'perdido', 'últimamente', '.']\n",
      "Tokens del post 32: ['¡hoy', 'es', 'un', 'buen', 'día', 'para', 'probar', 'algo', 'nuevo', '!']\n",
      "Tokens del post 33: ['las', 'vacaciones', 'están', 'a', 'la', 'vuelta', 'de', 'la', 'esquina', 'y', 'estoy', 'emocionado', '.']\n",
      "Tokens del post 34: ['extraño', 'a', 'mi', 'familia', 'que', 'está', 'lejos', '.']\n",
      "Tokens del post 35: ['¡acabo', 'de', 'terminar', 'de', 'decorar', 'mi', 'hogar', 'y', 'se', 've', 'increíble', '!']\n",
      "Tokens del post 36: ['estoy', 'emocionado', 'por', 'comenzar', 'este', 'nuevo', 'proyecto', '.']\n",
      "Tokens del post 37: ['la', 'playa', 'siempre', 'me', 'hace', 'sentir', 'en', 'paz', '.']\n",
      "Tokens del post 38: ['¡qué', 'lindo', 'cachorro', '!', 'me', 'encantaría', 'tener', 'uno', '.']\n",
      "Tokens del post 39: ['estoy', 'cansado', 'de', 'la', 'negatividad', 'en', 'el', 'mundo', '.']\n",
      "Tokens del post 40: ['la', 'comida', 'casera', 'siempre', 'es', 'la', 'mejor', '.']\n",
      "Tokens del post 41: ['mi', 'corazón', 'está', 'lleno', 'de', 'amor', 'por', 'mi', 'familia', 'y', 'amigos', '.']\n",
      "Tokens del post 42: ['¿alguien', 'más', 'ama', 'el', 'olor', 'a', 'café', 'por', 'la', 'mañana', '?']\n",
      "Tokens del post 43: ['estoy', 'cansado', 'de', 'la', 'política', 'en', 'las', 'redes', 'sociales', '.']\n",
      "Tokens del post 44: ['me', 'siento', 'inspirado', 'después', 'de', 'leer', 'un', 'libro', 'motivador', '.']\n",
      "Tokens del post 45: ['¡hoy', 'es', 'un', 'día', 'para', 'celebrar', 'los', 'éxitos', '!']\n",
      "Tokens del post 46: ['la', 'naturaleza', 'siempre', 'me', 'recuerda', 'la', 'belleza', 'de', 'la', 'vida', '.']\n",
      "Tokens del post 47: ['estoy', 'preocupado', 'por', 'el', 'cambio', 'climático', 'y', 'su', 'impacto', 'en', 'nuestro', 'planeta', '.']\n",
      "Tokens del post 48: ['¡acabo', 'de', 'ver', 'una', 'película', 'de', 'terror', 'y', 'todavía', 'estoy', 'temblando', '!']\n",
      "Tokens del post 49: ['la', 'música', 'siempre', 'está', 'ahí', 'para', 'mí', 'cuando', 'lo', 'necesito', '.']\n",
      "Tokens del post 50: ['¡estoy', 'tan', 'agradecido', 'por', 'el', 'apoyo', 'de', 'mis', 'amigos', 'y', 'familiares', '!']\n",
      "Tokens del post 51: ['la', 'tecnología', 'moderna', 'a', 'veces', 'puede', 'ser', 'abrumadora', '.']\n",
      "Tokens del post 52: ['¡el', 'arte', 'callejero', 'siempre', 'me', 'sorprende', '!']\n",
      "Tokens del post 53: ['estoy', 'cansado', 'de', 'la', 'negatividad', 'en', 'las', 'noticias', '.']\n",
      "Tokens del post 54: ['la', 'primavera', 'está', 'en', 'el', 'aire', 'y', 'estoy', 'emocionado', 'por', 'lo', 'que', 'está', 'por', 'venir', '.']\n",
      "Tokens del post 55: ['estoy', 'emocionado', 'por', 'mi', 'próximo', 'viaje', 'a', 'un', 'lugar', 'exótico', '.']\n",
      "Tokens del post 56: ['¡hoy', 'es', 'un', 'buen', 'día', 'para', 'hacer', 'algo', 'amable', 'por', 'alguien', 'más', '!']\n",
      "Tokens del post 57: ['estoy', 'tan', 'feliz', 'de', 'haber', 'encontrado', 'el', 'amor', 'verdadero', '.']\n",
      "Tokens del post 58: ['la', 'nostalgia', 'a', 'veces', 'puede', 'ser', 'abrumadora', '.']\n",
      "Tokens del post 59: ['¡acabo', 'de', 'ver', 'un', 'espectáculo', 'increíble', 'y', 'todavía', 'estoy', 'asombrado', '!']\n",
      "Tokens del post 60: ['la', 'soledad', 'a', 'veces', 'puede', 'ser', 'difícil', 'de', 'sobrellevar', '.']\n",
      "Tokens del post 61: ['¡estoy', 'tan', 'emocionado', 'por', 'el', 'concierto', 'de', 'esta', 'noche', '!']\n",
      "Tokens del post 62: ['¡hoy', 'es', 'un', 'buen', 'día', 'para', 'relajarse', 'y', 'tomar', 'las', 'cosas', 'con', 'calma', '!']\n",
      "Tokens del post 63: ['la', 'amistad', 'verdadera', 'es', 'una', 'de', 'las', 'cosas', 'más', 'preciosas', 'de', 'la', 'vida', '.']\n",
      "Tokens del post 64: ['estoy', 'cansado', 'de', 'la', 'negatividad', 'en', 'las', 'redes', 'sociales', '.']\n",
      "Tokens del post 65: ['¡acabo', 'de', 'leer', 'un', 'libro', 'que', 'me', 'dejó', 'sin', 'aliento', '!']\n",
      "Tokens del post 66: ['la', 'diversidad', 'es', 'lo', 'que', 'hace', 'que', 'nuestro', 'mundo', 'sea', 'hermoso', '.']\n",
      "Tokens del post 67: ['estoy', 'emocionado', 'por', 'el', 'futuro', 'y', 'las', 'oportunidades', 'que', 'vendrán', '.']\n",
      "Tokens del post 68: ['¡la', 'comida', 'picante', 'es', 'mi', 'favorita', '!', 'me', 'encanta', 'el', 'sabor', 'y', 'el', 'calor', '.']\n",
      "Tokens del post 69: ['estoy', 'cansado', 'de', 'la', 'violencia', 'en', 'el', 'mundo', '.']\n",
      "Tokens del post 70: ['¡hoy', 'es', 'un', 'buen', 'día', 'para', 'disfrutar', 'de', 'las', 'pequeñas', 'cosas', 'de', 'la', 'vida', '!']\n",
      "Tokens del post 71: ['me', 'siento', 'optimista', 'sobre', 'lo', 'que', 'el', 'futuro', 'traerá', '.']\n",
      "Tokens del post 72: ['la', 'música', 'siempre', 'me', 'ayuda', 'a', 'desconectar', 'del', 'mundo', '.']\n",
      "Tokens del post 73: ['estoy', 'emocionado', 'por', 'el', 'próximo', 'capítulo', 'de', 'mi', 'vida', '.']\n",
      "Tokens del post 74: ['la', 'danza', 'es', 'una', 'forma', 'increíble', 'de', 'expresión', '.']\n",
      "Tokens del post 75: ['¡hoy', 'es', 'un', 'día', 'perfecto', 'para', 'pasar', 'tiempo', 'al', 'aire', 'libre', '!']\n",
      "Tokens del post 76: ['la', 'tecnología', 'moderna', 'a', 'veces', 'puede', 'ser', 'intimidante', '.']\n",
      "Tokens del post 77: ['estoy', 'emocionado', 'por', 'mi', 'próxima', 'aventura', '.']\n",
      "Tokens del post 78: ['la', 'bondad', 'de', 'los', 'extraños', 'me', 'llena', 'de', 'esperanza', '.']\n",
      "Tokens del post 79: ['estoy', 'emocionado', 'por', 'el', 'crecimiento', 'personal', 'que', 'he', 'experimentado', 'últimamente', '.']\n",
      "Tokens del post 80: ['¡qué', 'maravilloso', 'es', 'encontrar', 'belleza', 'en', 'lugares', 'inesperados', '!']\n",
      "Tokens del post 81: ['la', 'música', 'siempre', 'me', 'ayuda', 'a', 'mantener', 'la', 'calma', 'en', 'tiempos', 'difíciles', '.']\n",
      "Tokens del post 82: ['estoy', 'cansado', 'de', 'la', 'negatividad', 'en', 'las', 'redes', 'sociales', '.']\n",
      "Tokens del post 83: ['¡hoy', 'es', 'un', 'buen', 'día', 'para', 'hacer', 'algo', 'amable', 'por', 'alguien', 'más', '!']\n",
      "Tokens del post 84: ['la', 'amistad', 'verdadera', 'es', 'una', 'de', 'las', 'cosas', 'más', 'preciosas', 'de', 'la', 'vida', '.']\n",
      "Tokens del post 85: ['estoy', 'emocionado', 'por', 'el', 'próximo', 'capítulo', 'de', 'mi', 'vida', '.']\n",
      "Tokens del post 86: ['la', 'danza', 'es', 'una', 'forma', 'increíble', 'de', 'expresión', '.']\n",
      "Tokens del post 87: ['¡hoy', 'es', 'un', 'día', 'perfecto', 'para', 'pasar', 'tiempo', 'al', 'aire', 'libre', '!']\n",
      "Tokens del post 88: ['la', 'tecnología', 'moderna', 'a', 'veces', 'puede', 'ser', 'intimidante', '.']\n",
      "Tokens del post 89: ['estoy', 'emocionado', 'por', 'mi', 'próxima', 'aventura', '.']\n",
      "Tokens del post 90: ['la', 'bondad', 'de', 'los', 'extraños', 'me', 'llena', 'de', 'esperanza', '.']\n",
      "Tokens del post 91: ['estoy', 'emocionado', 'por', 'el', 'crecimiento', 'personal', 'que', 'he', 'experimentado', 'últimamente', '.']\n",
      "Tokens del post 92: ['¡qué', 'maravilloso', 'es', 'encontrar', 'belleza', 'en', 'lugares', 'inesperados', '!']\n",
      "Tokens del post 93: ['la', 'música', 'siempre', 'me', 'ayuda', 'a', 'mantener', 'la', 'calma', 'en', 'tiempos', 'difíciles', '.']\n",
      "Tokens del post 94: ['estoy', 'cansado', 'de', 'la', 'negatividad', 'en', 'las', 'redes', 'sociales', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_posts = [\n",
    "    word_tokenize(post.lower()) for post in df_posts['post']\n",
    "]  # Convertir a minúsculas para consistencia\n",
    "\n",
    "# Mostrar los resultados\n",
    "for i, post in enumerate(tokenized_posts, start=1):\n",
    "    print(f\"Tokens del post {i}: {post}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_es = set(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens del post 1 después de eliminar stopwords: ['hoy', 'disfruté', 'delicioso', 'almuerzo', 'amigos', '.']\n",
      "Tokens del post 2 después de eliminar stopwords: ['tan', 'emocionado', 'próximo', 'viaje', '.']\n",
      "Tokens del post 3 después de eliminar stopwords: ['día', 'tan', 'hermoso', 'salir', 'caminar', 'parque', '.']\n",
      "Tokens del post 4 después de eliminar stopwords: ['corazón', 'roto', '.', 'puedo', 'creer', 'terminado', 'así', '.']\n",
      "Tokens del post 5 después de eliminar stopwords: ['¡acabo', 'conseguir', 'nuevo', 'trabajo', '!', '¡estoy', 'tan', 'feliz', '!']\n",
      "Tokens del post 6 después de eliminar stopwords: ['vida', 'veces', 'puede', 'ser', 'difícil', ',', 'sigo', 'adelante', '.']\n",
      "Tokens del post 7 después de eliminar stopwords: ['cansado', 'drama', 'redes', 'sociales', '.']\n",
      "Tokens del post 8 después de eliminar stopwords: ['¡felicidades', 'mejor', 'amigo', 'boda', '!']\n",
      "Tokens del post 9 después de eliminar stopwords: ['¡hoy', 'cumpleaños', 'emocionado', 'año', 'nuevo', 'traerá', '!']\n",
      "Tokens del post 10 después de eliminar stopwords: ['año', 'sido', 'desafío', ',', 'agradecido', 'todas', 'lecciones', 'aprendidas', '.']\n",
      "Tokens del post 11 después de eliminar stopwords: ['comida', 'restaurante', 'increíble', '.', 'definitivamente', 'volveré', '.']\n",
      "Tokens del post 12 después de eliminar stopwords: ['¿alguien', 'emocionado', 'nueva', 'temporada', 'serie', 'favorita', '?']\n",
      "Tokens del post 13 después de eliminar stopwords: ['perdí', 'mascota', 'hoy', '.', 'devastado', '.']\n",
      "Tokens del post 14 después de eliminar stopwords: ['¡finalmente', 'terminé', 'proyecto', 'orgulloso', 'resultado', 'final', '!']\n",
      "Tokens del post 15 después de eliminar stopwords: ['abrumado', 'hacer', '.']\n",
      "Tokens del post 16 después de eliminar stopwords: ['¡hoy', 'día', 'fiesta', 'nacional', 'país', '!', '¡vamos', 'celebrar', '!']\n",
      "Tokens del post 17 después de eliminar stopwords: ['¿alguien', 'recomendaciones', 'libros', 'interesantes', '?']\n",
      "Tokens del post 18 después de eliminar stopwords: ['siento', 'agradecido', 'pequeñas', 'cosas', 'vida', '.']\n",
      "Tokens del post 19 después de eliminar stopwords: ['música', 'siempre', 'levanta', 'ánimo', '.']\n",
      "Tokens del post 20 después de eliminar stopwords: ['odio', 'enfermo', '.', 'espero', 'recuperarme', 'pronto', '.']\n",
      "Tokens del post 21 después de eliminar stopwords: ['acabo', 'ver', 'película', 'dejó', 'pensando', 'horas', '.']\n",
      "Tokens del post 22 después de eliminar stopwords: ['¡hoy', 'día', 'descanso', 'bien', 'merecido', '!']\n",
      "Tokens del post 23 después de eliminar stopwords: ['risas', 'amigos', 'siempre', 'mejores', '.']\n",
      "Tokens del post 24 después de eliminar stopwords: ['nervioso', 'entrevista', 'trabajo', 'mañana', '.']\n",
      "Tokens del post 25 después de eliminar stopwords: ['lluvia', 'siempre', 'pone', 'buen', 'humor', '.']\n",
      "Tokens del post 26 después de eliminar stopwords: ['¡qué', 'hermoso', 'atardecer', '!', 'puedo', 'resistirme', 'tomar', 'foto', '.']\n",
      "Tokens del post 27 después de eliminar stopwords: ['noticias', 'hoy', 'dejado', 'palabras', '.']\n",
      "Tokens del post 28 después de eliminar stopwords: ['¡es', 'viernes', '!', '¡vamos', 'divertirnos', 'fin', 'semana', '!']\n",
      "Tokens del post 29 después de eliminar stopwords: ['libro', 'favorito', 'acaba', 'lanzar', 'nueva', 'secuela', '.', '¡no', 'puedo', 'esperar', 'leerlo', '!']\n",
      "Tokens del post 30 después de eliminar stopwords: ['corazón', 'lleno', 'gratitud', 'todas', 'personas', 'increíbles', 'vida', '.']\n",
      "Tokens del post 31 después de eliminar stopwords: ['siento', 'perdido', 'últimamente', '.']\n",
      "Tokens del post 32 después de eliminar stopwords: ['¡hoy', 'buen', 'día', 'probar', 'nuevo', '!']\n",
      "Tokens del post 33 después de eliminar stopwords: ['vacaciones', 'vuelta', 'esquina', 'emocionado', '.']\n",
      "Tokens del post 34 después de eliminar stopwords: ['extraño', 'familia', 'lejos', '.']\n",
      "Tokens del post 35 después de eliminar stopwords: ['¡acabo', 'terminar', 'decorar', 'hogar', 've', 'increíble', '!']\n",
      "Tokens del post 36 después de eliminar stopwords: ['emocionado', 'comenzar', 'nuevo', 'proyecto', '.']\n",
      "Tokens del post 37 después de eliminar stopwords: ['playa', 'siempre', 'hace', 'sentir', 'paz', '.']\n",
      "Tokens del post 38 después de eliminar stopwords: ['¡qué', 'lindo', 'cachorro', '!', 'encantaría', 'tener', '.']\n",
      "Tokens del post 39 después de eliminar stopwords: ['cansado', 'negatividad', 'mundo', '.']\n",
      "Tokens del post 40 después de eliminar stopwords: ['comida', 'casera', 'siempre', 'mejor', '.']\n",
      "Tokens del post 41 después de eliminar stopwords: ['corazón', 'lleno', 'amor', 'familia', 'amigos', '.']\n",
      "Tokens del post 42 después de eliminar stopwords: ['¿alguien', 'ama', 'olor', 'café', 'mañana', '?']\n",
      "Tokens del post 43 después de eliminar stopwords: ['cansado', 'política', 'redes', 'sociales', '.']\n",
      "Tokens del post 44 después de eliminar stopwords: ['siento', 'inspirado', 'después', 'leer', 'libro', 'motivador', '.']\n",
      "Tokens del post 45 después de eliminar stopwords: ['¡hoy', 'día', 'celebrar', 'éxitos', '!']\n",
      "Tokens del post 46 después de eliminar stopwords: ['naturaleza', 'siempre', 'recuerda', 'belleza', 'vida', '.']\n",
      "Tokens del post 47 después de eliminar stopwords: ['preocupado', 'cambio', 'climático', 'impacto', 'planeta', '.']\n",
      "Tokens del post 48 después de eliminar stopwords: ['¡acabo', 'ver', 'película', 'terror', 'todavía', 'temblando', '!']\n",
      "Tokens del post 49 después de eliminar stopwords: ['música', 'siempre', 'ahí', 'necesito', '.']\n",
      "Tokens del post 50 después de eliminar stopwords: ['¡estoy', 'tan', 'agradecido', 'apoyo', 'amigos', 'familiares', '!']\n",
      "Tokens del post 51 después de eliminar stopwords: ['tecnología', 'moderna', 'veces', 'puede', 'ser', 'abrumadora', '.']\n",
      "Tokens del post 52 después de eliminar stopwords: ['¡el', 'arte', 'callejero', 'siempre', 'sorprende', '!']\n",
      "Tokens del post 53 después de eliminar stopwords: ['cansado', 'negatividad', 'noticias', '.']\n",
      "Tokens del post 54 después de eliminar stopwords: ['primavera', 'aire', 'emocionado', 'venir', '.']\n",
      "Tokens del post 55 después de eliminar stopwords: ['emocionado', 'próximo', 'viaje', 'lugar', 'exótico', '.']\n",
      "Tokens del post 56 después de eliminar stopwords: ['¡hoy', 'buen', 'día', 'hacer', 'amable', 'alguien', '!']\n",
      "Tokens del post 57 después de eliminar stopwords: ['tan', 'feliz', 'haber', 'encontrado', 'amor', 'verdadero', '.']\n",
      "Tokens del post 58 después de eliminar stopwords: ['nostalgia', 'veces', 'puede', 'ser', 'abrumadora', '.']\n",
      "Tokens del post 59 después de eliminar stopwords: ['¡acabo', 'ver', 'espectáculo', 'increíble', 'todavía', 'asombrado', '!']\n",
      "Tokens del post 60 después de eliminar stopwords: ['soledad', 'veces', 'puede', 'ser', 'difícil', 'sobrellevar', '.']\n",
      "Tokens del post 61 después de eliminar stopwords: ['¡estoy', 'tan', 'emocionado', 'concierto', 'noche', '!']\n",
      "Tokens del post 62 después de eliminar stopwords: ['¡hoy', 'buen', 'día', 'relajarse', 'tomar', 'cosas', 'calma', '!']\n",
      "Tokens del post 63 después de eliminar stopwords: ['amistad', 'verdadera', 'cosas', 'preciosas', 'vida', '.']\n",
      "Tokens del post 64 después de eliminar stopwords: ['cansado', 'negatividad', 'redes', 'sociales', '.']\n",
      "Tokens del post 65 después de eliminar stopwords: ['¡acabo', 'leer', 'libro', 'dejó', 'aliento', '!']\n",
      "Tokens del post 66 después de eliminar stopwords: ['diversidad', 'hace', 'mundo', 'hermoso', '.']\n",
      "Tokens del post 67 después de eliminar stopwords: ['emocionado', 'futuro', 'oportunidades', 'vendrán', '.']\n",
      "Tokens del post 68 después de eliminar stopwords: ['¡la', 'comida', 'picante', 'favorita', '!', 'encanta', 'sabor', 'calor', '.']\n",
      "Tokens del post 69 después de eliminar stopwords: ['cansado', 'violencia', 'mundo', '.']\n",
      "Tokens del post 70 después de eliminar stopwords: ['¡hoy', 'buen', 'día', 'disfrutar', 'pequeñas', 'cosas', 'vida', '!']\n",
      "Tokens del post 71 después de eliminar stopwords: ['siento', 'optimista', 'futuro', 'traerá', '.']\n",
      "Tokens del post 72 después de eliminar stopwords: ['música', 'siempre', 'ayuda', 'desconectar', 'mundo', '.']\n",
      "Tokens del post 73 después de eliminar stopwords: ['emocionado', 'próximo', 'capítulo', 'vida', '.']\n",
      "Tokens del post 74 después de eliminar stopwords: ['danza', 'forma', 'increíble', 'expresión', '.']\n",
      "Tokens del post 75 después de eliminar stopwords: ['¡hoy', 'día', 'perfecto', 'pasar', 'tiempo', 'aire', 'libre', '!']\n",
      "Tokens del post 76 después de eliminar stopwords: ['tecnología', 'moderna', 'veces', 'puede', 'ser', 'intimidante', '.']\n",
      "Tokens del post 77 después de eliminar stopwords: ['emocionado', 'próxima', 'aventura', '.']\n",
      "Tokens del post 78 después de eliminar stopwords: ['bondad', 'extraños', 'llena', 'esperanza', '.']\n",
      "Tokens del post 79 después de eliminar stopwords: ['emocionado', 'crecimiento', 'personal', 'experimentado', 'últimamente', '.']\n",
      "Tokens del post 80 después de eliminar stopwords: ['¡qué', 'maravilloso', 'encontrar', 'belleza', 'lugares', 'inesperados', '!']\n",
      "Tokens del post 81 después de eliminar stopwords: ['música', 'siempre', 'ayuda', 'mantener', 'calma', 'tiempos', 'difíciles', '.']\n",
      "Tokens del post 82 después de eliminar stopwords: ['cansado', 'negatividad', 'redes', 'sociales', '.']\n",
      "Tokens del post 83 después de eliminar stopwords: ['¡hoy', 'buen', 'día', 'hacer', 'amable', 'alguien', '!']\n",
      "Tokens del post 84 después de eliminar stopwords: ['amistad', 'verdadera', 'cosas', 'preciosas', 'vida', '.']\n",
      "Tokens del post 85 después de eliminar stopwords: ['emocionado', 'próximo', 'capítulo', 'vida', '.']\n",
      "Tokens del post 86 después de eliminar stopwords: ['danza', 'forma', 'increíble', 'expresión', '.']\n",
      "Tokens del post 87 después de eliminar stopwords: ['¡hoy', 'día', 'perfecto', 'pasar', 'tiempo', 'aire', 'libre', '!']\n",
      "Tokens del post 88 después de eliminar stopwords: ['tecnología', 'moderna', 'veces', 'puede', 'ser', 'intimidante', '.']\n",
      "Tokens del post 89 después de eliminar stopwords: ['emocionado', 'próxima', 'aventura', '.']\n",
      "Tokens del post 90 después de eliminar stopwords: ['bondad', 'extraños', 'llena', 'esperanza', '.']\n",
      "Tokens del post 91 después de eliminar stopwords: ['emocionado', 'crecimiento', 'personal', 'experimentado', 'últimamente', '.']\n",
      "Tokens del post 92 después de eliminar stopwords: ['¡qué', 'maravilloso', 'encontrar', 'belleza', 'lugares', 'inesperados', '!']\n",
      "Tokens del post 93 después de eliminar stopwords: ['música', 'siempre', 'ayuda', 'mantener', 'calma', 'tiempos', 'difíciles', '.']\n",
      "Tokens del post 94 después de eliminar stopwords: ['cansado', 'negatividad', 'redes', 'sociales', '.']\n"
     ]
    }
   ],
   "source": [
    "# Eliminar stopwords de cada post\n",
    "filtered_posts = [\n",
    "    [word for word in post if word not in stopwords_es] for post in tokenized_posts\n",
    "]\n",
    "\n",
    "# Mostrar los resultados\n",
    "for i, post in enumerate(filtered_posts, start=1):\n",
    "    print(f\"Tokens del post {i} después de eliminar stopwords: {post}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens del post 1 después de lematización: ['hoy', 'disfruté', 'delicioso', 'almuerzo', 'amigo', '.']\n",
      "Tokens del post 2 después de lematización: ['tanto', 'emocionado', 'próximo', 'viaje', '.']\n",
      "Tokens del post 3 después de lematización: ['día', 'tanto', 'hermoso', 'salir', 'caminar', 'parque', '.']\n",
      "Tokens del post 4 después de lematización: ['corazón', 'roto', '.', 'poder', 'creer', 'terminado', 'así', '.']\n",
      "Tokens del post 5 después de lematización: ['¡', 'acabar', 'conseguir', 'nuevo', 'trabajo', '!', '¡', 'estar', 'tanto', 'feliz', '!']\n",
      "Tokens del post 6 después de lematización: ['vida', 'vez', 'poder', 'ser', 'difícil', ',', 'seguir', 'adelante', '.']\n",
      "Tokens del post 7 después de lematización: ['cansado', 'drama', 'red', 'social', '.']\n",
      "Tokens del post 8 después de lematización: ['¡', 'felicidad', 'mejor', 'amigo', 'boda', '!']\n",
      "Tokens del post 9 después de lematización: ['¡', 'hoy', 'cumpleaños', 'emocionado', 'año', 'nuevo', 'traer', '!']\n",
      "Tokens del post 10 después de lematización: ['año', 'ser', 'desafío', ',', 'agradecido', 'todo', 'lección', 'aprendido', '.']\n",
      "Tokens del post 11 después de lematización: ['comida', 'restaurante', 'increíble', '.', 'definitivamente', 'volver', '.']\n",
      "Tokens del post 12 después de lematización: ['¿', 'alguien', 'emocionado', 'nuevo', 'temporada', 'serie', 'favorito', '?']\n",
      "Tokens del post 13 después de lematización: ['perder', 'mascota', 'hoy', '.', 'devastado', '.']\n",
      "Tokens del post 14 después de lematización: ['¡', 'finalmente', 'terminar', 'proyecto', 'orgulloso', 'resultado', 'final', '!']\n",
      "Tokens del post 15 después de lematización: ['abrumar', 'hacer', '.']\n",
      "Tokens del post 16 después de lematización: ['¡', 'hoy', 'día', 'fiesta', 'nacional', 'país', '!', '¡', 'ir', 'celebrar', '!']\n",
      "Tokens del post 17 después de lematización: ['¿', 'alguien', 'recomendación', 'libro', 'interesante', '?']\n",
      "Tokens del post 18 después de lematización: ['sentir', 'agradecido', 'pequeño', 'cosa', 'vido', '.']\n",
      "Tokens del post 19 después de lematización: ['música', 'siempre', 'levantar', 'ánimo', '.']\n",
      "Tokens del post 20 después de lematización: ['odio', 'enfermo', '.', 'esperar', 'recuperar yo', 'pronto', '.']\n",
      "Tokens del post 21 después de lematización: ['acabar', 'ver', 'película', 'dejar', 'pensar', 'hora', '.']\n",
      "Tokens del post 22 después de lematización: ['¡', 'hoy', 'día', 'descanso', 'bien', 'merecido', '!']\n",
      "Tokens del post 23 después de lematización: ['risa', 'amigo', 'siempre', 'mejor', '.']\n",
      "Tokens del post 24 después de lematización: ['nervioso', 'entrevistar', 'trabajar', 'mañana', '.']\n",
      "Tokens del post 25 después de lematización: ['lluvia', 'siempre', 'poner', 'buen', 'humor', '.']\n",
      "Tokens del post 26 después de lematización: ['¡', 'qué', 'hermoso', 'atardecer', '!', 'poder', 'resistir yo', 'tomar', 'foto', '.']\n",
      "Tokens del post 27 después de lematización: ['noticia', 'hoy', 'dejar', 'palabra', '.']\n",
      "Tokens del post 28 después de lematización: ['¡', 'ser', 'viernes', '!', '¡', 'ir', 'divertirnos', 'fin', 'semana', '!']\n",
      "Tokens del post 29 después de lematización: ['libro', 'favorito', 'acabar', 'lanzar', 'nuevo', 'secuela', '.', '¡', 'no', 'poder', 'esperar', 'leer él', '!']\n",
      "Tokens del post 30 después de lematización: ['corazón', 'lleno', 'gratitud', 'todo', 'persona', 'increíble', 'vida', '.']\n",
      "Tokens del post 31 después de lematización: ['sentir', 'perdido', 'últimamente', '.']\n",
      "Tokens del post 32 después de lematización: ['¡', 'hoy', 'buen', 'día', 'probar', 'nuevo', '!']\n",
      "Tokens del post 33 después de lematización: ['vacación', 'vuelta', 'esquina', 'emocionado', '.']\n",
      "Tokens del post 34 después de lematización: ['extraño', 'familia', 'lejos', '.']\n",
      "Tokens del post 35 después de lematización: ['¡', 'acabar', 'terminar', 'decorar', 'hogar', 'ver', 'increíble', '!']\n",
      "Tokens del post 36 después de lematización: ['emocionado', 'comenzar', 'nuevo', 'proyecto', '.']\n",
      "Tokens del post 37 después de lematización: ['playa', 'siempre', 'hacer', 'sentir', 'paz', '.']\n",
      "Tokens del post 38 después de lematización: ['¡', 'qué', 'lindo', 'cachorro', '!', 'encantar', 'tener', '.']\n",
      "Tokens del post 39 después de lematización: ['cansado', 'negatividad', 'mundo', '.']\n",
      "Tokens del post 40 después de lematización: ['comida', 'casero', 'siempre', 'mejor', '.']\n",
      "Tokens del post 41 después de lematización: ['corazón', 'lleno', 'amor', 'familia', 'amigo', '.']\n",
      "Tokens del post 42 después de lematización: ['¿', 'alguien', 'amar', 'olor', 'café', 'mañana', '?']\n",
      "Tokens del post 43 después de lematización: ['cansado', 'política', 'red', 'social', '.']\n",
      "Tokens del post 44 después de lematización: ['sentir', 'inspirado', 'después', 'leer', 'libro', 'motivador', '.']\n",
      "Tokens del post 45 después de lematización: ['¡', 'hoy', 'día', 'celebrar', 'éxito', '!']\n",
      "Tokens del post 46 después de lematización: ['naturaleza', 'siempre', 'recordar', 'belleza', 'vido', '.']\n",
      "Tokens del post 47 después de lematización: ['preocupado', 'cambio', 'climático', 'impacto', 'planeta', '.']\n",
      "Tokens del post 48 después de lematización: ['¡', 'acabar', 'ver', 'película', 'terror', 'todavía', 'temblar', '!']\n",
      "Tokens del post 49 después de lematización: ['música', 'siempre', 'ahí', 'necesitar', '.']\n",
      "Tokens del post 50 después de lematización: ['¡', 'estar', 'tanto', 'agradecido', 'apoyo', 'amigo', 'familiar', '!']\n",
      "Tokens del post 51 después de lematización: ['tecnología', 'moderno', 'vez', 'poder', 'ser', 'abrumador', '.']\n",
      "Tokens del post 52 después de lematización: ['¡', 'el', 'arte', 'callejero', 'siempre', 'sorprender', '!']\n",
      "Tokens del post 53 después de lematización: ['cansado', 'negatividad', 'noticia', '.']\n",
      "Tokens del post 54 después de lematización: ['primavera', 'aire', 'emocionado', 'venir', '.']\n",
      "Tokens del post 55 después de lematización: ['emocionado', 'próximo', 'viaje', 'lugar', 'exótico', '.']\n",
      "Tokens del post 56 después de lematización: ['¡', 'hoy', 'buen', 'día', 'hacer', 'amable', 'alguien', '!']\n",
      "Tokens del post 57 después de lematización: ['tanto', 'feliz', 'haber', 'encontrar', 'amor', 'verdadero', '.']\n",
      "Tokens del post 58 después de lematización: ['nostalgia', 'vez', 'poder', 'ser', 'abrumador', '.']\n",
      "Tokens del post 59 después de lematización: ['¡', 'acabar', 'ver', 'espectáculo', 'increíble', 'todavía', 'asombrado', '!']\n",
      "Tokens del post 60 después de lematización: ['soledad', 'vez', 'poder', 'ser', 'difícil', 'sobrellevar', '.']\n",
      "Tokens del post 61 después de lematización: ['¡', 'estar', 'tanto', 'emocionado', 'concierto', 'noche', '!']\n",
      "Tokens del post 62 después de lematización: ['¡', 'hoy', 'buen', 'día', 'relajar él', 'tomar', 'cosa', 'calmo', '!']\n",
      "Tokens del post 63 después de lematización: ['amistad', 'verdadero', 'cosa', 'precioso', 'vida', '.']\n",
      "Tokens del post 64 después de lematización: ['cansado', 'negatividad', 'red', 'social', '.']\n",
      "Tokens del post 65 después de lematización: ['¡', 'acabar', 'leer', 'libro', 'dejar', 'aliento', '!']\n",
      "Tokens del post 66 después de lematización: ['diversidad', 'hacer', 'mundo', 'hermoso', '.']\n",
      "Tokens del post 67 después de lematización: ['emocionado', 'futuro', 'oportunidad', 'venir', '.']\n",
      "Tokens del post 68 después de lematización: ['¡', 'el', 'comida', 'picante', 'favorito', '!', 'encantar', 'sabor', 'calor', '.']\n",
      "Tokens del post 69 después de lematización: ['cansado', 'violencia', 'mundo', '.']\n",
      "Tokens del post 70 después de lematización: ['¡', 'hoy', 'buen', 'día', 'disfrutar', 'pequeño', 'cosa', 'vida', '!']\n",
      "Tokens del post 71 después de lematización: ['sentir', 'optimista', 'futuro', 'traer', '.']\n",
      "Tokens del post 72 después de lematización: ['música', 'siempre', 'ayudar', 'desconectar', 'mundo', '.']\n",
      "Tokens del post 73 después de lematización: ['emocionado', 'próximo', 'capítulo', 'vida', '.']\n",
      "Tokens del post 74 después de lematización: ['danzar', 'forma', 'increíble', 'expresión', '.']\n",
      "Tokens del post 75 después de lematización: ['¡', 'hoy', 'día', 'perfecto', 'pasar', 'tiempo', 'aire', 'libre', '!']\n",
      "Tokens del post 76 después de lematización: ['tecnología', 'moderno', 'vez', 'poder', 'ser', 'intimidante', '.']\n",
      "Tokens del post 77 después de lematización: ['emocionado', 'próximo', 'aventura', '.']\n",
      "Tokens del post 78 después de lematización: ['bondad', 'extraño', 'lleno', 'esperanza', '.']\n",
      "Tokens del post 79 después de lematización: ['emocionado', 'crecimiento', 'personal', 'experimentado', 'últimamente', '.']\n",
      "Tokens del post 80 después de lematización: ['¡', 'qué', 'maravilloso', 'encontrar', 'belleza', 'lugar', 'inesperado', '!']\n",
      "Tokens del post 81 después de lematización: ['música', 'siempre', 'ayudar', 'mantener', 'calmo', 'tiempo', 'difícil', '.']\n",
      "Tokens del post 82 después de lematización: ['cansado', 'negatividad', 'red', 'social', '.']\n",
      "Tokens del post 83 después de lematización: ['¡', 'hoy', 'buen', 'día', 'hacer', 'amable', 'alguien', '!']\n",
      "Tokens del post 84 después de lematización: ['amistad', 'verdadero', 'cosa', 'precioso', 'vida', '.']\n",
      "Tokens del post 85 después de lematización: ['emocionado', 'próximo', 'capítulo', 'vida', '.']\n",
      "Tokens del post 86 después de lematización: ['danzar', 'forma', 'increíble', 'expresión', '.']\n",
      "Tokens del post 87 después de lematización: ['¡', 'hoy', 'día', 'perfecto', 'pasar', 'tiempo', 'aire', 'libre', '!']\n",
      "Tokens del post 88 después de lematización: ['tecnología', 'moderno', 'vez', 'poder', 'ser', 'intimidante', '.']\n",
      "Tokens del post 89 después de lematización: ['emocionado', 'próximo', 'aventura', '.']\n",
      "Tokens del post 90 después de lematización: ['bondad', 'extraño', 'lleno', 'esperanza', '.']\n",
      "Tokens del post 91 después de lematización: ['emocionado', 'crecimiento', 'personal', 'experimentado', 'últimamente', '.']\n",
      "Tokens del post 92 después de lematización: ['¡', 'qué', 'maravilloso', 'encontrar', 'belleza', 'lugar', 'inesperado', '!']\n",
      "Tokens del post 93 después de lematización: ['música', 'siempre', 'ayudar', 'mantener', 'calmo', 'tiempo', 'difícil', '.']\n",
      "Tokens del post 94 después de lematización: ['cansado', 'negatividad', 'red', 'social', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lematizar cada token en cada post\n",
    "lemmatized_posts = [\n",
    "    [token.lemma_ for token in nlp(\" \".join(post))] for post in filtered_posts\n",
    "]\n",
    "\n",
    "# Mostrar los resultados\n",
    "for i, post in enumerate(lemmatized_posts, start=1):\n",
    "    print(f\"Tokens del post {i} después de lematización: {post}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz TF-IDF:\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(94, 227)\n",
      "Post 1:   (0, 12)\t0.38983613802156125\n",
      "  (0, 9)\t0.5036759584510153\n",
      "  (0, 54)\t0.5036759584510153\n",
      "  (0, 62)\t0.5036759584510153\n",
      "  (0, 100)\t0.2948887453393017\n",
      "Post 2:   (0, 215)\t0.6084304337452373\n",
      "  (0, 170)\t0.49271972952185983\n",
      "  (0, 68)\t0.3798248854043043\n",
      "  (0, 195)\t0.49271972952185983\n",
      "Post 3:   (0, 146)\t0.46201643830011996\n",
      "  (0, 35)\t0.46201643830011996\n",
      "  (0, 183)\t0.46201643830011996\n",
      "  (0, 97)\t0.39613221707862833\n",
      "  (0, 66)\t0.2917081970533469\n",
      "  (0, 195)\t0.3429402667895249\n",
      "Post 4:   (0, 19)\t0.4392668537855498\n",
      "  (0, 200)\t0.4392668537855498\n",
      "  (0, 48)\t0.4392668537855498\n",
      "  (0, 161)\t0.2938210523964493\n",
      "  (0, 181)\t0.4392668537855498\n",
      "  (0, 45)\t0.37662675665706574\n",
      "Post 5:   (0, 86)\t0.4024084818565733\n",
      "  (0, 77)\t0.37642445703423677\n",
      "  (0, 208)\t0.43903090793924077\n",
      "  (0, 139)\t0.3258788307435239\n",
      "  (0, 44)\t0.43903090793924077\n",
      "  (0, 2)\t0.31381800612923266\n",
      "  (0, 195)\t0.3258788307435239\n",
      "Post 6:   (0, 3)\t0.4640029356366644\n",
      "  (0, 185)\t0.4640029356366644\n",
      "  (0, 60)\t0.3765342601987832\n",
      "  (0, 188)\t0.32042442602652277\n",
      "  (0, 214)\t0.3444147812658446\n",
      "  (0, 216)\t0.3316679383305755\n",
      "  (0, 161)\t0.31036676154573883\n",
      "Post 7:   (0, 192)\t0.4732284455271403\n",
      "  (0, 175)\t0.4732284455271403\n",
      "  (0, 65)\t0.6114204600856771\n",
      "  (0, 36)\t0.4222258846595644\n",
      "Post 8:   (0, 26)\t0.5476529754046563\n",
      "  (0, 124)\t0.4695568584852615\n",
      "  (0, 85)\t0.5476529754046563\n",
      "  (0, 12)\t0.42387355863548043\n",
      "Post 9:   (0, 209)\t0.464046821918188\n",
      "  (0, 23)\t0.464046821918188\n",
      "  (0, 49)\t0.5062788354090288\n",
      "  (0, 139)\t0.37579485163746223\n",
      "  (0, 68)\t0.2896905236517306\n",
      "  (0, 100)\t0.2964126598870235\n",
      "Post 10:   (0, 16)\t0.4119636691568821\n",
      "  (0, 110)\t0.4119636691568821\n",
      "  (0, 205)\t0.3775990976663246\n",
      "  (0, 4)\t0.3532170461713199\n",
      "  (0, 55)\t0.4119636691568821\n",
      "  (0, 23)\t0.3775990976663246\n",
      "  (0, 188)\t0.28448790319020495\n",
      "Post 11:   (0, 220)\t0.48302444718834636\n",
      "  (0, 52)\t0.48302444718834636\n",
      "  (0, 103)\t0.3585338508605858\n",
      "  (0, 178)\t0.48302444718834636\n",
      "  (0, 42)\t0.4141444531105741\n",
      "Post 12:   (0, 84)\t0.41774394192950326\n",
      "  (0, 189)\t0.4872225985431822\n",
      "  (0, 198)\t0.4872225985431822\n",
      "  (0, 7)\t0.377101533210019\n",
      "  (0, 139)\t0.36165000653449925\n",
      "  (0, 68)\t0.2787866287021451\n",
      "Post 13:   (0, 59)\t0.5469481786912727\n",
      "  (0, 122)\t0.5469481786912727\n",
      "  (0, 153)\t0.5469481786912727\n",
      "  (0, 100)\t0.3202234680327146\n",
      "Post 14:   (0, 89)\t0.41958142376388574\n",
      "  (0, 179)\t0.41958142376388574\n",
      "  (0, 144)\t0.41958142376388574\n",
      "  (0, 169)\t0.3845814057706654\n",
      "  (0, 201)\t0.3845814057706654\n",
      "  (0, 90)\t0.41958142376388574\n",
      "Post 15:   (0, 96)\t0.6120686005334565\n",
      "  (0, 1)\t0.7908046713576091\n",
      "Post 16:   (0, 39)\t0.3936456516028134\n",
      "  (0, 108)\t0.3936456516028134\n",
      "  (0, 149)\t0.4294705892683315\n",
      "  (0, 130)\t0.4294705892683315\n",
      "  (0, 87)\t0.4294705892683315\n",
      "  (0, 66)\t0.271159380700483\n",
      "  (0, 100)\t0.2514434947797607\n",
      "Post 17:   (0, 106)\t0.5540556498387645\n",
      "  (0, 115)\t0.4496112377710366\n",
      "  (0, 172)\t0.5540556498387645\n",
      "  (0, 7)\t0.4288291135563035\n",
      "Post 18:   (0, 217)\t0.48218006553803605\n",
      "  (0, 46)\t0.4071628033242685\n",
      "  (0, 152)\t0.48218006553803605\n",
      "  (0, 187)\t0.4071628033242685\n",
      "  (0, 4)\t0.4510450886260888\n",
      "Post 19:   (0, 223)\t0.5775727175219705\n",
      "  (0, 113)\t0.5775727175219705\n",
      "  (0, 190)\t0.3646681852174528\n",
      "  (0, 129)\t0.4470309012124126\n",
      "Post 20:   (0, 168)\t0.41958142376388574\n",
      "  (0, 222)\t0.3845814057706654\n",
      "  (0, 174)\t0.41958142376388574\n",
      "  (0, 75)\t0.3845814057706654\n",
      "  (0, 71)\t0.41958142376388574\n",
      "  (0, 140)\t0.41958142376388574\n",
      "Post 21:   (0, 99)\t0.45908716725846943\n",
      "  (0, 151)\t0.45908716725846943\n",
      "  (0, 53)\t0.3936206643805845\n",
      "  (0, 150)\t0.420791718021553\n",
      "  (0, 212)\t0.3725451578301689\n",
      "  (0, 2)\t0.3281541615026995\n",
      "Post 22:   (0, 125)\t0.5169895398070589\n",
      "  (0, 25)\t0.5169895398070589\n",
      "  (0, 56)\t0.5169895398070589\n",
      "  (0, 66)\t0.32641714460945725\n",
      "  (0, 100)\t0.3026834896311085\n",
      "Post 23:   (0, 180)\t0.6049149819052358\n",
      "  (0, 190)\t0.38193155938644113\n",
      "  (0, 124)\t0.5186532189370743\n",
      "  (0, 12)\t0.46819332235460276\n",
      "Post 24:   (0, 123)\t0.46773442289424527\n",
      "  (0, 207)\t0.5103020378951363\n",
      "  (0, 72)\t0.5103020378951363\n",
      "  (0, 134)\t0.5103020378951363\n",
      "Post 25:   (0, 101)\t0.5031798290852456\n",
      "  (0, 28)\t0.3734945567402268\n",
      "  (0, 163)\t0.5031798290852456\n",
      "  (0, 118)\t0.5031798290852456\n",
      "  (0, 190)\t0.31769796173512066\n",
      "Post 26:   (0, 92)\t0.39159076664230164\n",
      "  (0, 206)\t0.35892563157623586\n",
      "  (0, 177)\t0.39159076664230164\n",
      "  (0, 20)\t0.39159076664230164\n",
      "  (0, 171)\t0.31777242834901254\n",
      "  (0, 222)\t0.35892563157623586\n",
      "  (0, 161)\t0.2619310111200528\n",
      "  (0, 97)\t0.33574934941334195\n",
      "Post 27:   (0, 145)\t0.5854026067276836\n",
      "  (0, 138)\t0.5365703643825673\n",
      "  (0, 53)\t0.5019233370566968\n",
      "  (0, 100)\t0.3427375027928242\n",
      "Post 28:   (0, 186)\t0.43367703375130584\n",
      "  (0, 88)\t0.43367703375130584\n",
      "  (0, 64)\t0.43367703375130584\n",
      "  (0, 218)\t0.43367703375130584\n",
      "  (0, 108)\t0.39750120916789017\n",
      "  (0, 188)\t0.29948240398517595\n",
      "Post 29:   (0, 224)\t0.31779978537752207\n",
      "  (0, 111)\t0.29727905113831193\n",
      "  (0, 135)\t0.34672213585925005\n",
      "  (0, 184)\t0.34672213585925005\n",
      "  (0, 109)\t0.34672213585925005\n",
      "  (0, 75)\t0.31779978537752207\n",
      "  (0, 115)\t0.2813619330687458\n",
      "  (0, 84)\t0.29727905113831193\n",
      "  (0, 139)\t0.2573609333271513\n",
      "  (0, 2)\t0.2478359664173737\n",
      "  (0, 161)\t0.23191884834780765\n",
      "Post 30:   (0, 156)\t0.43454967335397715\n",
      "  (0, 94)\t0.43454967335397715\n",
      "  (0, 117)\t0.35263319951080413\n",
      "  (0, 103)\t0.32255255129366056\n",
      "  (0, 205)\t0.39830105622049783\n",
      "  (0, 216)\t0.31061483278285995\n",
      "  (0, 45)\t0.3725822530684186\n",
      "Post 31:   (0, 226)\t0.5611972392103524\n",
      "  (0, 154)\t0.6545348710992684\n",
      "  (0, 187)\t0.5065982246492211\n",
      "Post 32:   (0, 167)\t0.5930416452280913\n",
      "  (0, 28)\t0.4401961557474038\n",
      "  (0, 139)\t0.4401961557474038\n",
      "  (0, 66)\t0.37443496543874416\n",
      "  (0, 100)\t0.34720995465634297\n",
      "Post 33:   (0, 76)\t0.5482100390831413\n",
      "  (0, 221)\t0.5482100390831413\n",
      "  (0, 210)\t0.5482100390831413\n",
      "  (0, 68)\t0.31368337403404456\n",
      "Post 34:   (0, 112)\t0.6231457913614057\n",
      "  (0, 82)\t0.5711651442812764\n",
      "  (0, 80)\t0.5342842882461698\n",
      "Post 35:   (0, 98)\t0.4682651516246876\n",
      "  (0, 51)\t0.4682651516246876\n",
      "  (0, 212)\t0.37999301060874713\n",
      "  (0, 201)\t0.42920410696393785\n",
      "  (0, 103)\t0.34757849010145486\n",
      "  (0, 2)\t0.3347145578256173\n",
      "Post 36:   (0, 41)\t0.6065068019587276\n",
      "  (0, 169)\t0.55591412130299\n",
      "  (0, 139)\t0.45019091796529603\n",
      "  (0, 68)\t0.3470405254365624\n",
      "Post 37:   (0, 148)\t0.527285238016352\n",
      "  (0, 160)\t0.527285238016352\n",
      "  (0, 190)\t0.33291764829951714\n",
      "  (0, 187)\t0.40810929601688634\n",
      "  (0, 96)\t0.40810929601688634\n",
      "Post 38:   (0, 199)\t0.4714756075878399\n",
      "  (0, 69)\t0.43214675789542595\n",
      "  (0, 29)\t0.4714756075878399\n",
      "  (0, 116)\t0.4714756075878399\n",
      "  (0, 171)\t0.3825982671020657\n",
      "Post 39:   (0, 128)\t0.6161738824509032\n",
      "  (0, 133)\t0.5876928279593556\n",
      "  (0, 36)\t0.5243537799525388\n",
      "Post 40:   (0, 38)\t0.5903938812632474\n",
      "  (0, 190)\t0.3727632187466801\n",
      "  (0, 42)\t0.5062028485284003\n",
      "  (0, 124)\t0.5062028485284003\n",
      "Post 41:   (0, 14)\t0.4782605848553443\n",
      "  (0, 82)\t0.4782605848553443\n",
      "  (0, 117)\t0.4234248380804801\n",
      "  (0, 45)\t0.4473786937702162\n",
      "  (0, 12)\t0.40385311290693576\n",
      "Post 42:   (0, 30)\t0.47462317684327376\n",
      "  (0, 141)\t0.47462317684327376\n",
      "  (0, 11)\t0.47462317684327376\n",
      "  (0, 123)\t0.43503176790887327\n",
      "  (0, 7)\t0.36734980729500283\n",
      "Post 43:   (0, 162)\t0.6114204600856771\n",
      "  (0, 192)\t0.4732284455271403\n",
      "  (0, 175)\t0.4732284455271403\n",
      "  (0, 36)\t0.4222258846595644\n",
      "Post 44:   (0, 127)\t0.4475404976438606\n",
      "  (0, 58)\t0.4475404976438606\n",
      "  (0, 105)\t0.4475404976438606\n",
      "  (0, 111)\t0.38372056677553307\n",
      "  (0, 187)\t0.34638830041894475\n",
      "  (0, 115)\t0.3631751380152491\n",
      "Post 45:   (0, 225)\t0.6223864195161596\n",
      "  (0, 39)\t0.5704691165850841\n",
      "  (0, 66)\t0.3929626854307096\n",
      "  (0, 100)\t0.36439053182481834\n",
      "Post 46:   (0, 24)\t0.43010483730393584\n",
      "  (0, 173)\t0.5016393427737132\n",
      "  (0, 131)\t0.5016393427737132\n",
      "  (0, 190)\t0.31672532862670605\n",
      "  (0, 217)\t0.4597943395662609\n",
      "Post 47:   (0, 159)\t0.447213595499958\n",
      "  (0, 102)\t0.447213595499958\n",
      "  (0, 40)\t0.447213595499958\n",
      "  (0, 34)\t0.447213595499958\n",
      "  (0, 165)\t0.447213595499958\n",
      "Post 48:   (0, 197)\t0.45409050035243403\n",
      "  (0, 204)\t0.41621185562999913\n",
      "  (0, 202)\t0.45409050035243403\n",
      "  (0, 150)\t0.41621185562999913\n",
      "  (0, 212)\t0.36849040702489166\n",
      "  (0, 2)\t0.3245825586442489\n",
      "Post 49:   (0, 132)\t0.5775727175219705\n",
      "  (0, 5)\t0.5775727175219705\n",
      "  (0, 190)\t0.3646681852174528\n",
      "  (0, 129)\t0.4470309012124126\n",
      "Post 50:   (0, 83)\t0.4652282466200045\n",
      "  (0, 15)\t0.4652282466200045\n",
      "  (0, 4)\t0.3988860168249503\n",
      "  (0, 77)\t0.3988860168249503\n",
      "  (0, 195)\t0.34532429105963647\n",
      "  (0, 12)\t0.36007830018061765\n",
      "Post 51:   (0, 0)\t0.4710879585369386\n",
      "  (0, 126)\t0.4406692130083826\n",
      "  (0, 196)\t0.4406692130083826\n",
      "  (0, 188)\t0.3549235852802729\n",
      "  (0, 214)\t0.38149691178748085\n",
      "  (0, 161)\t0.34378304152918443\n",
      "Post 52:   (0, 194)\t0.4857135764531404\n",
      "  (0, 31)\t0.4857135764531404\n",
      "  (0, 17)\t0.4857135764531404\n",
      "  (0, 67)\t0.44519704508978375\n",
      "  (0, 190)\t0.30667010938567674\n",
      "Post 53:   (0, 133)\t0.559148262422484\n",
      "  (0, 138)\t0.6621679181377286\n",
      "  (0, 36)\t0.49888562699186195\n",
      "Post 54:   (0, 211)\t0.5379891858566574\n",
      "  (0, 6)\t0.5032505434330357\n",
      "  (0, 166)\t0.5869505524297712\n",
      "  (0, 68)\t0.3358505254395642\n",
      "Post 55:   (0, 81)\t0.5380990506259483\n",
      "  (0, 119)\t0.46136533738199387\n",
      "  (0, 215)\t0.49321270583714455\n",
      "  (0, 170)\t0.39941399630673025\n",
      "  (0, 68)\t0.3078979108940846\n",
      "Post 56:   (0, 10)\t0.5022390792787228\n",
      "  (0, 28)\t0.4067237428842001\n",
      "  (0, 96)\t0.4241010487025063\n",
      "  (0, 7)\t0.4241010487025063\n",
      "  (0, 66)\t0.34596301812628977\n",
      "  (0, 100)\t0.3208081908046379\n",
      "Post 57:   (0, 213)\t0.3954262972924324\n",
      "  (0, 70)\t0.3954262972924324\n",
      "  (0, 95)\t0.46119311080671616\n",
      "  (0, 14)\t0.4227219911089387\n",
      "  (0, 86)\t0.4227219911089387\n",
      "  (0, 195)\t0.34232913669363063\n",
      "Post 58:   (0, 137)\t0.5492033960920415\n",
      "  (0, 0)\t0.5033907655596274\n",
      "  (0, 188)\t0.37926092584553905\n",
      "  (0, 214)\t0.4076564025957275\n",
      "  (0, 161)\t0.3673564677799415\n",
      "Post 59:   (0, 18)\t0.4682651516246876\n",
      "  (0, 73)\t0.4682651516246876\n",
      "  (0, 204)\t0.42920410696393785\n",
      "  (0, 212)\t0.37999301060874713\n",
      "  (0, 103)\t0.34757849010145486\n",
      "  (0, 2)\t0.3347145578256173\n",
      "Post 60:   (0, 191)\t0.49184312872160685\n",
      "  (0, 193)\t0.49184312872160685\n",
      "  (0, 60)\t0.3991263295628413\n",
      "  (0, 188)\t0.33964990329095013\n",
      "  (0, 214)\t0.3650796807208292\n",
      "  (0, 161)\t0.3289887785739839\n",
      "Post 61:   (0, 136)\t0.5260606434699739\n",
      "  (0, 43)\t0.5260606434699739\n",
      "  (0, 77)\t0.45104362472965387\n",
      "  (0, 68)\t0.3010095872490136\n",
      "  (0, 195)\t0.3904782654115688\n",
      "Post 62:   (0, 32)\t0.37219095252470047\n",
      "  (0, 176)\t0.43409329218698495\n",
      "  (0, 224)\t0.3978827447776445\n",
      "  (0, 206)\t0.3978827447776445\n",
      "  (0, 28)\t0.3222137939114733\n",
      "  (0, 46)\t0.33598040511536004\n",
      "  (0, 66)\t0.27407806545307545\n",
      "  (0, 100)\t0.2541499631765258\n",
      "Post 63:   (0, 164)\t0.4881687032093283\n",
      "  (0, 13)\t0.4881687032093283\n",
      "  (0, 213)\t0.4566470323857992\n",
      "  (0, 46)\t0.4122197326264284\n",
      "  (0, 216)\t0.38069806180289917\n",
      "Post 64:   (0, 133)\t0.513255052018905\n",
      "  (0, 192)\t0.513255052018905\n",
      "  (0, 175)\t0.513255052018905\n",
      "  (0, 36)\t0.4579385927514882\n",
      "Post 65:   (0, 8)\t0.5241627822566743\n",
      "  (0, 111)\t0.4494164013068294\n",
      "  (0, 53)\t0.4494164013068294\n",
      "  (0, 115)\t0.4253534412879207\n",
      "  (0, 2)\t0.3746700203569844\n",
      "Post 66:   (0, 63)\t0.578054165309374\n",
      "  (0, 128)\t0.4690858198794369\n",
      "  (0, 96)\t0.44740353314560566\n",
      "  (0, 97)\t0.4956227941543326\n",
      "Post 67:   (0, 142)\t0.5766147475228937\n",
      "  (0, 93)\t0.5285155577222166\n",
      "  (0, 211)\t0.5285155577222166\n",
      "  (0, 68)\t0.32993642331555095\n",
      "Post 68:   (0, 33)\t0.4032220340551771\n",
      "  (0, 182)\t0.4032220340551771\n",
      "  (0, 158)\t0.4032220340551771\n",
      "  (0, 67)\t0.3695866592557058\n",
      "  (0, 69)\t0.3695866592557058\n",
      "  (0, 84)\t0.34572198104664276\n",
      "  (0, 42)\t0.34572198104664276\n",
      "Post 69:   (0, 219)\t0.6843220391955169\n",
      "  (0, 128)\t0.5553212554844151\n",
      "  (0, 36)\t0.4725692011531245\n",
      "Post 70:   (0, 61)\t0.4855003300181161\n",
      "  (0, 28)\t0.36037162079211643\n",
      "  (0, 46)\t0.3757685283302295\n",
      "  (0, 152)\t0.4450015869281198\n",
      "  (0, 216)\t0.34703421282233543\n",
      "  (0, 66)\t0.30653546973233914\n",
      "  (0, 100)\t0.28424740307469515\n",
      "Post 71:   (0, 143)\t0.5522166632276527\n",
      "  (0, 93)\t0.5061526764673612\n",
      "  (0, 187)\t0.42740577097597876\n",
      "  (0, 209)\t0.5061526764673612\n",
      "Post 72:   (0, 57)\t0.5430181619991409\n",
      "  (0, 22)\t0.4655829762640409\n",
      "  (0, 128)\t0.44065441444309095\n",
      "  (0, 190)\t0.34285111063752927\n",
      "  (0, 129)\t0.42028629637262943\n",
      "Post 73:   (0, 37)\t0.613868088628023\n",
      "  (0, 216)\t0.4787246499149866\n",
      "  (0, 170)\t0.4971232565226168\n",
      "  (0, 68)\t0.3832194503835885\n",
      "Post 74:   (0, 79)\t0.5230078298961983\n",
      "  (0, 91)\t0.5230078298961983\n",
      "  (0, 50)\t0.5230078298961983\n",
      "  (0, 103)\t0.4235427128422912\n",
      "Post 75:   (0, 114)\t0.424806717423866\n",
      "  (0, 203)\t0.3760998109327903\n",
      "  (0, 147)\t0.424806717423866\n",
      "  (0, 155)\t0.424806717423866\n",
      "  (0, 6)\t0.3973764101914969\n",
      "  (0, 66)\t0.2926244096563451\n",
      "  (0, 100)\t0.27134781039763867\n",
      "Post 76:   (0, 107)\t0.4710879585369386\n",
      "  (0, 126)\t0.4406692130083826\n",
      "  (0, 196)\t0.4406692130083826\n",
      "  (0, 188)\t0.3549235852802729\n",
      "  (0, 214)\t0.38149691178748085\n",
      "  (0, 161)\t0.34378304152918443\n",
      "Post 77:   (0, 21)\t0.6991941838195948\n",
      "  (0, 170)\t0.5662221184667113\n",
      "  (0, 68)\t0.4364859744274956\n",
      "Post 78:   (0, 74)\t0.5227898121208174\n",
      "  (0, 27)\t0.5227898121208174\n",
      "  (0, 80)\t0.48903261249038404\n",
      "  (0, 117)\t0.46284849422482804\n",
      "Post 79:   (0, 78)\t0.4842322618270538\n",
      "  (0, 157)\t0.4842322618270538\n",
      "  (0, 47)\t0.4842322618270538\n",
      "  (0, 226)\t0.4529647720041755\n",
      "  (0, 68)\t0.3022916888383997\n",
      "Post 80:   (0, 104)\t0.4299767536282021\n",
      "  (0, 121)\t0.4299767536282021\n",
      "  (0, 70)\t0.40221261061671115\n",
      "  (0, 119)\t0.40221261061671115\n",
      "  (0, 24)\t0.40221261061671115\n",
      "  (0, 171)\t0.38067706821054276\n",
      "Post 81:   (0, 120)\t0.4261973654395036\n",
      "  (0, 203)\t0.37733101193386465\n",
      "  (0, 22)\t0.3986772623052421\n",
      "  (0, 32)\t0.3986772623052421\n",
      "  (0, 190)\t0.29358234543731276\n",
      "  (0, 129)\t0.3598898554384082\n",
      "  (0, 60)\t0.37733101193386465\n",
      "Post 82:   (0, 133)\t0.513255052018905\n",
      "  (0, 192)\t0.513255052018905\n",
      "  (0, 175)\t0.513255052018905\n",
      "  (0, 36)\t0.4579385927514882\n",
      "Post 83:   (0, 10)\t0.5022390792787228\n",
      "  (0, 28)\t0.4067237428842001\n",
      "  (0, 96)\t0.4241010487025063\n",
      "  (0, 7)\t0.4241010487025063\n",
      "  (0, 66)\t0.34596301812628977\n",
      "  (0, 100)\t0.3208081908046379\n",
      "Post 84:   (0, 164)\t0.4881687032093283\n",
      "  (0, 13)\t0.4881687032093283\n",
      "  (0, 213)\t0.4566470323857992\n",
      "  (0, 46)\t0.4122197326264284\n",
      "  (0, 216)\t0.38069806180289917\n",
      "Post 85:   (0, 37)\t0.613868088628023\n",
      "  (0, 216)\t0.4787246499149866\n",
      "  (0, 170)\t0.4971232565226168\n",
      "  (0, 68)\t0.3832194503835885\n",
      "Post 86:   (0, 79)\t0.5230078298961983\n",
      "  (0, 91)\t0.5230078298961983\n",
      "  (0, 50)\t0.5230078298961983\n",
      "  (0, 103)\t0.4235427128422912\n",
      "Post 87:   (0, 114)\t0.424806717423866\n",
      "  (0, 203)\t0.3760998109327903\n",
      "  (0, 147)\t0.424806717423866\n",
      "  (0, 155)\t0.424806717423866\n",
      "  (0, 6)\t0.3973764101914969\n",
      "  (0, 66)\t0.2926244096563451\n",
      "  (0, 100)\t0.27134781039763867\n",
      "Post 88:   (0, 107)\t0.4710879585369386\n",
      "  (0, 126)\t0.4406692130083826\n",
      "  (0, 196)\t0.4406692130083826\n",
      "  (0, 188)\t0.3549235852802729\n",
      "  (0, 214)\t0.38149691178748085\n",
      "  (0, 161)\t0.34378304152918443\n",
      "Post 89:   (0, 21)\t0.6991941838195948\n",
      "  (0, 170)\t0.5662221184667113\n",
      "  (0, 68)\t0.4364859744274956\n",
      "Post 90:   (0, 74)\t0.5227898121208174\n",
      "  (0, 27)\t0.5227898121208174\n",
      "  (0, 80)\t0.48903261249038404\n",
      "  (0, 117)\t0.46284849422482804\n",
      "Post 91:   (0, 78)\t0.4842322618270538\n",
      "  (0, 157)\t0.4842322618270538\n",
      "  (0, 47)\t0.4842322618270538\n",
      "  (0, 226)\t0.4529647720041755\n",
      "  (0, 68)\t0.3022916888383997\n",
      "Post 92:   (0, 104)\t0.4299767536282021\n",
      "  (0, 121)\t0.4299767536282021\n",
      "  (0, 70)\t0.40221261061671115\n",
      "  (0, 119)\t0.40221261061671115\n",
      "  (0, 24)\t0.40221261061671115\n",
      "  (0, 171)\t0.38067706821054276\n",
      "Post 93:   (0, 120)\t0.4261973654395036\n",
      "  (0, 203)\t0.37733101193386465\n",
      "  (0, 22)\t0.3986772623052421\n",
      "  (0, 32)\t0.3986772623052421\n",
      "  (0, 190)\t0.29358234543731276\n",
      "  (0, 129)\t0.3598898554384082\n",
      "  (0, 60)\t0.37733101193386465\n",
      "Post 94:   (0, 133)\t0.513255052018905\n",
      "  (0, 192)\t0.513255052018905\n",
      "  (0, 175)\t0.513255052018905\n",
      "  (0, 36)\t0.4579385927514882\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Convertir los posts lematizados de nuevo a texto\n",
    "lemmatized_posts_text = [\" \".join(post) for post in lemmatized_posts]\n",
    "\n",
    "# Crear un objeto TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Aplicar TF-IDF a los posts lematizados\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(lemmatized_posts_text)\n",
    "\n",
    "# Mostrar la matriz TF-IDF\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(type(tfidf_matrix))\n",
    "print(tfidf_matrix.shape)\n",
    "for i, post in enumerate(tfidf_matrix, start=1):\n",
    "    print(f\"Post {i}: {post}\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz TF-IDF normalizada:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "normalized_tfidf_matrix = normalize(tfidf_matrix, norm=\"l2\", axis=1)\n",
    "\n",
    "# Mostrar la matriz normalizada\n",
    "print(\"Matriz TF-IDF normalizada:\")\n",
    "print(normalized_tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (75, 227)\n",
      "Forma de X_test: (19, 227)\n",
      "Forma de y_train: 75\n",
      "Forma de y_test: 19\n"
     ]
    }
   ],
   "source": [
    "labels_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "labels = [labels_mapping[label] for label in df_posts['sentimiento']]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    normalized_tfidf_matrix, labels, test_size=0.2, random_state=42,shuffle=True\n",
    ")\n",
    "\n",
    "# Mostrar las formas de los conjuntos de datos\n",
    "print(\"Forma de X_train:\", X_train.shape)\n",
    "print(\"Forma de X_test:\", X_test.shape)\n",
    "print(\"Forma de y_train:\", len(y_train))\n",
    "print(\"Forma de y_test:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 227)\n",
      "(75,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.toarray().shape)\n",
    "print(np.array(y_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcofura/anaconda3/envs/analisis_sentimiento/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.4125 - loss: 1.0879 - val_accuracy: 0.4667 - val_loss: 1.0841\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5340 - loss: 1.0698 - val_accuracy: 0.5333 - val_loss: 1.0706\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6958 - loss: 1.0517 - val_accuracy: 0.6000 - val_loss: 1.0575\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8368 - loss: 1.0314 - val_accuracy: 0.6667 - val_loss: 1.0445\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8799 - loss: 1.0110 - val_accuracy: 0.6667 - val_loss: 1.0318\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8799 - loss: 1.0004 - val_accuracy: 0.6667 - val_loss: 1.0196\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8903 - loss: 0.9723 - val_accuracy: 0.6667 - val_loss: 1.0074\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8590 - loss: 0.9655 - val_accuracy: 0.6667 - val_loss: 0.9957\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8903 - loss: 0.9422 - val_accuracy: 0.6667 - val_loss: 0.9841\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8806 - loss: 0.9295 - val_accuracy: 0.6667 - val_loss: 0.9725\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8421 - loss: 0.9242\n",
      "Precisión en el conjunto de prueba: 0.8421052694320679\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Definir el modelo de red neuronal\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(\n",
    "            3, activation=\"softmax\"\n",
    "        ),  # Capa de salida con 3 neuronas para 3 clases de polaridad\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # Usar esta función de pérdida para etiquetas numéricas\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train.toarray(), np.array(y_train), epochs=10, validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_acc = model.evaluate(X_test.toarray(), np.array(y_test))\n",
    "print(\"Precisión en el conjunto de prueba:\", test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encontrar', 'estar']\n",
      "tfdidf new text    (0, 77)\t0.7071067811865476\n",
      "  (0, 70)\t0.7071067811865476\n",
      "tfdidf shape  (1, 227)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicción: [[0.30586234 0.31609133 0.37804633]]\n",
      "El sentimiento de la frase es: [2]\n"
     ]
    }
   ],
   "source": [
    "# Definir la frase a predecir\n",
    "frase = \"Detesto encontrar las empresas que estan afectando a los pobladores del campo.\"\n",
    "\n",
    "tokenized_frase=word_tokenize(frase.lower())\n",
    "tokenized_frase_stop = [word for word in tokenized_frase if word not in stopwords_es]\n",
    "lemmatized_frase=[token.lemma_ for token in nlp(\" \".join(tokenized_frase_stop))]\n",
    "\n",
    "lemmatized_frase = [\n",
    "    word for word in lemmatized_frase if word in tfidf_vectorizer.vocabulary_\n",
    "]\n",
    "\n",
    "# Tokenizar y lematizar el nuevo texto\n",
    "print(lemmatized_frase)\n",
    "# Convertir el texto tokenizado en una matriz TF-IDF usando el mismo vectorizador\n",
    "tfidf_new_text = tfidf_vectorizer.transform([\" \".join(lemmatized_frase)])\n",
    "print('tfdidf new text ',tfidf_new_text)\n",
    "print('tfdidf shape ',tfidf_new_text.shape)\n",
    "# Ahora puedes usar tfidf_new_text para hacer una predicción con tu modelo\n",
    "prediction = model.predict(tfidf_new_text)\n",
    "\n",
    "# Mapear el resultado de la predicción al sentimiento correspondiente\n",
    "print(\"Predicción:\", prediction)\n",
    "# Imprimir el resultado\n",
    "print(\"El sentimiento de la frase es:\", np.argmax(prediction, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analisis_sentimiento",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
